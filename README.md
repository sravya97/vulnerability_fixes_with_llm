# vulnerability_fixes_with_llm
## Overview

This repository contains research and experiments aimed at utilizing Large Language Models (LLMs) to automatically fix vulnerabilities in code. Explored various LLM types and training methodologies to enhance vulnerability mitigation techniques.

## Features

- Utilized multiple LLM models.
- Experimentation with training the models using datasets on BigML Server and TAMU HPRC Grace.
- Incorporation of the Juliet and AutoVAS datasets for training

# Usage

To replicate or extend our research, follow these steps:

1. Clone this repository to your local machine.
2. Explore the datasets in the `data/` directory and preprocess them as needed.
3. Train LLM models using the provided notebooks in the `LLM/` directory.
4. Analyze model performance and experiment with different configurations.


## Workflow

### Step 1: Data Preparation
- **Script Used**: `AutoVAS_Dataset_Formatting.py`
- **Objective**: Generate a CSV file containing pairs of vulnerable and patched code snippets from the AutoVAS dataset.
- **Details**: This script processes files in the NVD directory, extracting code snippets from both vulnerable and patched files, and saves them in a structured CSV format.

### Step 2: Training the LLM
- **Current Script**: `LLM_Train_Final.ipynb`
- **Objective**: Use the data generated in the first step to train an LLM.
- **Steps**:
  1. Load the CSV file with vulnerable and patched code.
  2. Split the data into training and testing datasets.
  3. Form a custom dataset with instructions to train the LLM.

## Model Training Attempts

### Models Tried
- **Google Gemma**
- **NexaAIDev/Octopus-v2**

### Challenges
- Both models have billions of parameters, resulting in out-of-memory errors on the available GPU.
- Attempted to train on the TAMU HPRC computing cluster but faced similar memory issues.

### Alternative Approach
- Recommendations included using more GPUs or implementing distributed training.

## Current Solution
- **Model Used**: `Salesforce/codegen-350M-mono`
- **Parameters**: 350 million
- **Outcome**: Successfully trained on a single GPU.
- **Results**: Mixed performanceâ€”some code vulnerabilities were fixed correctly, while others were not.

## Next Steps and To-Do List

### Evaluation Methodology
- **Challenge**: Evaluating the performance of code output is complex since it's not a simple yes/no check.
- **Suggestion**: Run the model's output through static analyzers to verify if the vulnerabilities have been fixed.

### Automated Test Case Generation
- **Goal**: Use a custom dataset to automatically generate test cases for both training and testing phases.
- **Benefit**: Improves the robustness of the model and provides a more comprehensive evaluation of its performance.